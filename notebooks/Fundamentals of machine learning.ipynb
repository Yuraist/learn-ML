{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning\n",
    "\n",
    "## Evaluating machine-learning models\n",
    "\n",
    "#### Simple hold-out validation\n",
    "\n",
    "```\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "training_data = data[:]\n",
    "\n",
    "model = get_model()\n",
    "model.train(training_data\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# Tune the model, retrain it, evaluate it ...\n",
    "\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "                            validation_data]))\n",
    "\n",
    "test_score = model.evaluate(test_data)\n",
    "```\n",
    "\n",
    "#### K-fold validation\n",
    "\n",
    "```\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "\n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold: \n",
    "    num_validation_samples * (fold + 1)]\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "        data[num_validation_samples * (fold + 1):]\n",
    "    \n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "validation_score = np.average(validation_scores)\n",
    "\n",
    "model.get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n",
    "```\n",
    "\n",
    "#### Iterated K-folld validation with shuffling\n",
    "\n",
    "It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. \n",
    "\n",
    "## Data preprocessing, feature engineering and feature learning\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "All inputs and targets in a neural network must be tensors of floating-poing data. Whatever data you need to process, you must first turn into tensors, a step called _data vectorization_. \n",
    "\n",
    "#### Value normalization\n",
    "\n",
    "Before you feed data into your network you have to normalize each feature independentrly so that it had a standard deviation of 1 and a mean of 0.\n",
    "\n",
    "* Take small values (the 0-1 range)\n",
    "* Be homogenous (all features in the same range)\n",
    "\n",
    "Additionaly:\n",
    "* Normalize each feature independently to have a mean of 0.\n",
    "* Normalize each feature independently to have a stddev of 1.\n",
    "\n",
    "```\n",
    "x -= x.mean(axis=0)\n",
    "x /- x.std(axis=0)\n",
    "```\n",
    "\n",
    "#### Handling missing values\n",
    "\n",
    "If you're expecting missing values in the test data, but the network was trained on data without any missing values, the network won't have learned to ignore missing values. In this situation, you should artificially generate training samples with missing entries: copy some training samples several times, and drop some of the features that you expect are likely to be missing in the test data.\n",
    "\n",
    "### Feature engineering \n",
    "\n",
    "Feature engineering is the process of using your own knowledge about the data and about the machine-learining algorithms at hand to make the algorithm work better by applying hardcoded transformations to the data before it goes into the model. \n",
    "\n",
    "## Overfitting and underfitting \n",
    "\n",
    "Optimization refers to the process of adjusting a model to get the best perfomance possible on the training data, whereas generalization refers to how well the trained model performs on data it has never seen before. \n",
    "\n",
    "A model trained on more data will naturally generalize better. When that isn't possible, the next-best solution is to constraints on what information that your model is allowed to store or to add constraints on what information it's allowed to store.\n",
    "\n",
    "The process of fighting overfitting this way is called regularization. \n",
    "\n",
    "### Reducing the network's size\n",
    "\n",
    "The simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model. \n",
    "\n",
    "The general workflow to find an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until you see diminishing returns with regard to validation loss. \n",
    "\n",
    "### Adding weight regularization\n",
    "\n",
    "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called weight regularization, and it's done by adding to the loss function of the network a cost associated with having large weights.\n",
    "\n",
    "* L1 regularization – The cost added is proportional to the absolute value of the weight coefficuients (the L1 norm of the weights).\n",
    "* L2 regularization – The cost added is proportional to the square of the value of the weight coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                      activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), \n",
    "                      activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# regularizers.l1(0.001) – L1 regilarization\n",
    "# regularizers.l1_l2(l1=0.001, l2=0.001) – Simultaneous L1 and L2 regilarizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout \n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural networks. Dropout, applied to a layer, consists of randomly dropping out a number of output features of the layer during training. The _dropout rate_ is the fraction of the features that are zeroed out; it's usually set netween 0.2 and 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recup:\n",
    "* Get more traning data\n",
    "* Reduce the capacity of the network\n",
    "* Add weight regularization\n",
    "* Add dropout\n",
    "\n",
    "## Choosing the right last-layer activation and loss function for your model\n",
    "\n",
    "| Problem Type | Last-layer Activation | Loss Function |\n",
    "|-----|-----|-----|\n",
    "| Binary classification | `sigmoid` | `binary_crossentropy` | \n",
    "| Multiclass, single-label classification | `softmax` | `categorical_crossentropy` | \n",
    "| Multiclass, multilabel classification | `sigmoid` | `binary_classification` |\n",
    "| Regression to arbitrary values | None | `mse` |\n",
    "| Regression to values between 0 and 1 | `sigmoid` | `mse` or `binary_crossentropy` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter summary\n",
    "* Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be.\n",
    "* Choose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?\n",
    "* Determine your evaluation protocol: hold-out validation? K-fold valida- tion? Which portion of the data should you use for validation?\n",
    "* Develop a first model that does better than a basic baseline: a model with statistical power.\n",
    "* Develop a model that overfits.\n",
    "* Regularize your model and tune its hyperparameters, based on perfor- mance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
