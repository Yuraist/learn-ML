{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets\n",
    "\n",
    "This is a theoretical article, so the most of the code doesn't actually work. I will update it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the `fully_connected()` function uses Xavier initialization. We can change this to He initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 784), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden_1 = tf.contrib.layers.fully_connected(X, n_hidden1, weights_initializer=he_init, scope='h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow offers an `elu()` function that we can use to build our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = tf.contrib.layers.fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow does not have a predefined function for leaky ReLUs, but it is easy to define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.contrib.layers.fully_connected(X, n_hidden1, activation_fn=leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Batch Normalization with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import batch_norm, fully_connected\n",
    "\n",
    "n_inputs = 28*28 \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "\n",
    "# tell the batch_norm() function wheter it should use the current\n",
    "# mini-batch's mean and stddev or the running avg\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99, # compute the running averages\n",
    "    'updates_collections': None,\n",
    "    # for non ReLU: \n",
    "    # 'scale': True \n",
    "}\n",
    "\n",
    "hidden_1 = fully_connected(X, n_hidden1, scope='hidden1',\n",
    "                          normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "hidden_2 = fully_connected(hidden_1, n_hidden2, scope='hidden2',\n",
    "                          normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "logits = fully_connected(hidden_2, n_outputs, activation_fn=None, scope='outputs',\n",
    "                        normalizer_fn=batch_norm, normalizer_params=bn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over and over, we can create an argument scope using the `arg_scope()` function: the first parameter is a list of functions, and the other parameters will be passed to these functions automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.contrib.framework.arg_scope(\n",
    "    [fully_connected], \n",
    "    normalizer_fn=batch_norm, \n",
    "    normalizer_params=bn_params):\n",
    "    hidden_1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    hidden_2 = fully_connected(hidden_1, n_hidden2, scope='hidden2')\n",
    "    logits = fully_connected(hidden_2, n_outputs, scope='outputs', activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution phase is also pretty much the same, with one exception. Whenever you run an operation that depends on the batch_norm layer, you need to set the is_train ing placeholder to True or False:\n",
    "\n",
    "`sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "In TensorFlow, the optimier's `minimize()` function takes care of both computing the gradients and applying them, so we must instead call the optimizer's `compute_gradients()` method first, then create an operation to clip the gradients using the `clip_by_value()` function, and finally create an operation to apply the clipped gradients using the optimizer's `apply_gradients()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, threshold, threshold), var) \n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... construct the original model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my_original_model.ckpt')\n",
    "    # train it on out new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we will want to reuse only part of the original model. A simple solution is to configure the `Saver` to restore only a subset of the variables from the original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... build new model with the same definition as before for hidden layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                              scope='hidden[123]')\n",
    "reuse_vars_dict = dict([(var.name, var. name) for var in reuse_vars])\n",
    "original_saver = tf.Saver(reuse_vars_dict) # saver to restore the original model\n",
    "\n",
    "new_saver = tf.Saver() # saver to save the new model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    original_saver.restore('./my_original_model.ckpt') # restore layers 1 to 3\n",
    "    # ... train the new model\n",
    "    new_saver.save('./my_new_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Models from Other Frameworks\n",
    "\n",
    "If the model was trained using another framework, we will need to load weights manually, then assign them to the appropriate variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Lower Layers\n",
    "\n",
    "A good idea to \"freeze\" the low level layer weights when training the new DNN: if the lower-layer weights are fixed, then the higher-layer weights will be easier to train. \n",
    "\n",
    "To freeze the lower layers during training, the simplest solution is to give the optimizer the list of variables to train, excluding the variables from the lower layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = tf.get_collection(tf.GrapthKeys.TRAINABLE_VARIABLES, \n",
    "                              scope='hidden[34]|outputs')\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\ell_1 $ and $ \\ell_2 $ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... construct the neural net\n",
    "with arg_scope(\n",
    "    [fully_connected], \n",
    "    weights_regularizer=tf.contrib.layers.l1_regularizer(scale=0.01)):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Don't forget to add the regilarization losses to your overall loss__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([base_loss] + reg_losses, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "X_drop = tf.contrib.layers.dropout(X, keep_prob, is_training=is_training)\n",
    "\n",
    "hidden_1 = fully_connected(X_drop, n_hidden1, scope='hidden1')\n",
    "hidden1_drop = tf.conttib.layers.dropout(hidden_1, keep_prob, is_training=is_training)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
